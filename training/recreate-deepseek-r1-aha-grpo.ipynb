{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreating Deepseek R1 „aha moment“ a RL tutorial \n",
    "\n",
    "The release of Deepseek R1 shocked the industry. Why? Well, DeepSeek-R1 is an open model that rivals OpenAI's o1 in complex reasoning tasks, introduced using Group Relative Policy Optimization (GRPO) and RL-focused multi-stage training approach. They not only released the model, but also a research paper on how they did it. \n",
    "\n",
    "In the paper they described an \"aha moment\" when using pure RL to train the model. During this phase, DeepSeek-R1-Zero (the first test of DeepSeek-R1) learns to allocate more thinking time to a problem by reevaluating its initial approach without any human feedback or data describing how to do it.  They describe this as an \"aha moment\" as:\n",
    " \n",
    "> This behavior is not only a testament to the model’s growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes.\n",
    "\n",
    "It serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future.\n",
    "\n",
    "In this blog post we will recreate the \"aha moment\" of DeepSeek-R1 using Group Relative Policy Optimization (GRPO) and the Countdown Game. We will train an open model using reinforcement learning to teach it self-verification and search abilities all on its own. \n",
    "\n",
    "You will learn how to:\n",
    "1. [Setup the development environment](#1-setup-the-development-environment)\n",
    "2. [Generate training samples with reasoning prefix from the Countdown Game](#2-generate-training-samples-with-reasoning-prefix-from-the-countdown-game)\n",
    "3. [Train the model using GRPO](#3-train-the-model-using-grpo)\n",
    "4. [Explore the results and generate CoT](#4-explore-the-results-and-generate-cot)\n",
    "\n",
    "_Note: This blog is inspired by [Jiayi Pan](https://x.com/jiayi_pirate/status/1882839370505621655) who initially explored the idea and proofed it with a small model._\n",
    "\n",
    "But Before we start, let's take a look at the [Group Relative Policy Optimization (GRPO)](https://arxiv.org/abs/2402.03300) and understand how it works.\n",
    "\n",
    "## Group Relative Policy Optimization (GRPO)\n",
    "\n",
    "Group Relative Policy Optimization (GRPO) is a reinforcement learning algorithm to improve the reasoning capabilities of LLMs. It was introduced in the [DeepSeekMath](https://arxiv.org/abs/2402.03300) paper in the context of mathematical reasoning. GRPO modifies the traditional Proximal Policy Optimization (PPO) by eliminating the need for a value function model. Instead, it estimates baselines from group scores, reducing memory usage and computational overhead. GRPO, now also used by the Qwen team, can be used with rule/binary-based Rewards as well as General Reward Models to improve models on helpfulness. \n",
    "\n",
    "1. **Sampling**: Generate multiple outputs for each prompt using the current policy\n",
    "2. **Reward Scoring**: Each generation is scored using a reward function, could be (rule-based or outcome-based)\n",
    "3. **Advantage Calculation**: The average reward of the generated outputs is used as a baseline. The advantage of each solution within the group is then computed relative to this baseline. The reward is normalized within a group.\n",
    "4. **Policy Optimization**: The policy tries to maximize the GRPO objective, which includes the calculated advantages and a KL divergence term. This is different from how PPO implements the KL term within the reward.\n",
    "\n",
    "![grpo.png](/static/blog/deepseek-r1/grpo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup development environment\n",
    "\n",
    "Our first step is to install Hugging Face Libraries and Pytorch, vllm, and trl, transformers and datasets. If you haven't heard of trl yet, don't worry. It is a new library on top of transformers and datasets, which makes it easier to fine-tune, rlhf, align open LLMs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Pytorch & other libraries, make sure to match your GPU driver version\n",
    "%pip install \"torch==2.5.1\" vllm tensorboard  \"setuptools<71.0.0\"  --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Install flash-attn\n",
    "%pip install flash-attn \n",
    "\n",
    "# Install Hugging Face libraries\n",
    "%pip install  --upgrade \\\n",
    "  \"transformers==4.48.1\" \\\n",
    "  \"datasets==3.1.0\" \\\n",
    "  \"accelerate==1.3.0\" \\\n",
    "  \"bitsandbytes==0.45.0\" \\\n",
    "  \"peft==0.14.0\" \\\n",
    "  \"hf-transfer==0.1.9\" \n",
    "  \n",
    "# Install TRL from main branch \n",
    "%pip install git+https://github.com/huggingface/trl.git@main --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: you may need to restart the kernel to use updated packages._\n",
    "\n",
    "We will use the [Hugging Face Hub](https://huggingface.co/models) as a remote model versioning service. This means we will automatically push our model, logs and information to the Hub during training. You must register on the [Hugging Face](https://huggingface.co/join) for this. After you have an account, we will use the `login` util from the `huggingface_hub` package to log into our account and store our token (access key) on the disk.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"\", add_to_git_credential=True) # ADD YOUR TOKEN HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate training samples with reasoning prefix from the Countdown Game\n",
    "\n",
    "The Countdown game is a numbers puzzle where players use a set of randomly drawn numbers and basic arithmetic operations (+, -, ×, ÷) to reach or get as close as possible to a target number.\n",
    "\n",
    "```\n",
    "Target Number: 952\n",
    "Available Numbers: 25, 50, 75, 100, 3, 6\n",
    "\n",
    "(100 × (3 × 3)) + (50 + 6 / 3) = 952\n",
    "```\n",
    "\n",
    "We are going to use the [Jiayi-Pan/Countdown-Tasks-3to4](https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-3to4) dataset, which contains samples with 3 to 4 numbers and solutions.\n",
    "\n",
    "As Model we are going to use [Qwen/Qwen2.5-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct) which is a 3B parameter instruction tuned model. This makes it easier to showcase the \"aha moment\" as it already follows the prompt format. But you can use the base version of Qwen or other models as well. [Jiayi-Pan](https://x.com/jiayi_pirate/status/1882839487417561307) explored that the model needs to have a certain quality to be able to learn the reasoning process, starting with > 1.5B parameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/dpo/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from Hugging Face Hub\n",
    "dataset_id = \"Jiayi-Pan/Countdown-Tasks-3to4\"\n",
    "dataset = load_dataset(dataset_id, split=\"train\")\n",
    "# select a random subset of 50k samples\n",
    "dataset = dataset.shuffle(seed=42).select(range(50000))\n",
    "\n",
    "# Load tokenizer from Hugging Face Hub to format the dataset to our \"r1\" prompt \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")\n",
    "\n",
    "# gemerate r1 prompt with a prefix for the model to already start with the thinking process\n",
    "def generate_r1_prompt(numbers, target):\n",
    "    r1_prefix = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer.\"\n",
    "      },\n",
    "      { \n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Using the numbers {numbers}, create an equation that equals {target}. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 </answer>.\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Let me solve this step by step.\\n<think>\"\n",
    "      }]\n",
    "    return {\"prompt\": tokenizer.apply_chat_template(r1_prefix, tokenize=False, continue_final_message=True), \"target\": target}\n",
    "\n",
    "# convert our dataset to the r1 prompt\n",
    "dataset = dataset.map(lambda x: generate_r1_prompt(x[\"nums\"], x[\"target\"]), remove_columns=dataset.features.keys())\n",
    "\n",
    "# split the dataset into train and test\n",
    "train_test_split = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the first sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the model using GRPO\n",
    "\n",
    "TRL supports Group Relative Policy Optimization (GRPO) through a dedicated [GRPOTrainer](https://huggingface.co/docs/trl/main/en/grpo_trainer) for aligning LLMs from preference data, as described in [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300). The `GRPOTrainer` is a subclass of the `Trainer` from the `transformers` library and supports all the same features, including logging, checkpointing, distributed training, and parameter efficient fine-tuning (PEFT). \n",
    "\n",
    "The `GRPOTrainer` supports generic Outcome Reward Models (ORM) and custom reward functions, that can be used to implement Rule-Based Reward Models. In the Deepseek R1 paper they implemented Rule-Based Reward Models to verify the correctness of the generated solutions. In our exmaple we are going to do a similar approach, where we will reward functions that: \n",
    "1. Checks if the generated format is correct `<think> [thinking] </think>\\n<answer> [answer] </answer>`\n",
    "2. Extracts the equation, splitting it into parts, executing the equation and comparing the result and comparing it to the target. \n",
    "\n",
    "_Note: Correct `<answer>` in our example includes the equation and the result, for example `<answer> 55 + 36 - 7 - 19 = 65 </answer>`_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target': []}\n"
     ]
    }
   ],
   "source": [
    "reward_kwargs = {key: [] for key in {\"prompt\":\"1\",\"completion\":\"2\",\"target\":3}.keys() if key not in [\"prompt\", \"completion\"]}\n",
    "print(reward_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "# Custom reward function to train R1 like reasoning model on the Countdown Game\n",
    "def reward_func(completions, target, **kwargs):\n",
    "    \"\"\"\n",
    "    Evaluates completions based on:\n",
    "    1. Format: <think>...</think><answer>...</answer>\n",
    "    2. Mathematical correctness of the answer\n",
    "\n",
    "    Args:\n",
    "        completions (list[str]): Generated outputs\n",
    "        ground_truth (list[str]): Expected answers\n",
    "    \n",
    "    Returns:\n",
    "        list[float]: Reward scores\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "\n",
    "    for completion, gt in zip(completions, target):\n",
    "        # Check if the format is correct\n",
    "        regex = r\"<think>\\s*(.*?)\\s*</think>\\s*<answer>\\s*(.*?)\\s*</answer>\"\n",
    "\n",
    "        match = re.search(regex, completion, re.DOTALL)  # Use re.DOTALL here\n",
    "        if match is None or len(match.groups()) != 2:\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "\n",
    "        # Extract the \"answer\" part from the completion\n",
    "        answer_equation = match.group(2)\n",
    "        if not answer_equation:\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "\n",
    "        # Check the answer contains an equation\n",
    "        if \"=\" not in answer_equation.strip():\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "\n",
    "        # Split the equation into the equation and target\n",
    "        equation, answer_target = map(str.strip, answer_equation.split(\"=\"))\n",
    "        try:\n",
    "            # Replace '×' with '*' for Python compatibility\n",
    "            equation = equation.replace(\"×\", \"*\")\n",
    "            # Evaluate the equation and compare it to the target\n",
    "            result = eval(equation)\n",
    "            # Check if the equation is correct and if it matches the ground truth\n",
    "            if float(result) == float(answer_target) and float(result) == float(gt):\n",
    "                rewards.append(1.0)\n",
    "            else:\n",
    "                rewards.append(0.0)\n",
    "        except Exception:\n",
    "            # If evaluation fails, reward is 0\n",
    "            rewards.append(0.0)\n",
    "\n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try our reward function with a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_sample_1 = \"\"\"Let me solve this step by step.\n",
    "<think> We need to find an equation using the numbers 19, 36, 55, and 7\n",
    "exactly once, with basic arithmetic operations, that equals 65. One possible\n",
    "combination is 55 + 36 - 19 + 7... </think>\n",
    "<answer> 55 + 36 - 7 - 19 = 65 </answer>\"\"\"\n",
    "\n",
    "correct_sample_2 = \"\"\"<think> ... </think>\n",
    "<answer> 55 + 36 - 7 - 19 = 65 </answer>\"\"\"\n",
    "\n",
    "wrong_format = \"\"\"User: Using the numbers [19, 36, 55, 7], create an equation that equals 65.\"\"\"\n",
    "\n",
    "wrong_result = \"\"\"<think> ... </think>\n",
    "<answer> 55 + 36 - 7 - 19 </answer>\"\"\"\n",
    "\n",
    "\n",
    "test_rewards = reward_func(completions=[correct_sample_1, correct_sample_2, wrong_format, wrong_result], target=[\"65\", \"65\", \"65\", \"65\"])\n",
    "assert test_rewards == [1.0, 1.0, 0.0, 0.0], \"Reward function is not working\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good, now lets define our remaining training parameters, create a trainer and start training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "from trl import GRPOConfig, GRPOTrainer, get_peft_config, ModelConfig\n",
    "\n",
    "# our model we are going to use as policy \n",
    "model_config = ModelConfig(\n",
    "    model_name_or_path=\"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    use_peft=True,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"qwen-r1-aha-moment\",\n",
    "    learning_rate=1e-5,\n",
    "    beta=0.04, # KL coefficient\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=10,\n",
    "    max_steps=100,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    bf16=True,\n",
    "    # GRPO specific parameters\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=1024, # max length of the generated output for our solution\n",
    "    num_generations=2,\n",
    "    \n",
    ")\n",
    "trainer = GRPOTrainer(\n",
    "    model=model_config.model_name_or_path,\n",
    "    reward_funcs=reward_func,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    peft_config=get_peft_config(model_config),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    }
   ],
   "source": [
    "# Train and push the model to the Hub\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and push to hub\n",
    "trainer.save_model(training_args.output_dir)\n",
    "if training_args.push_to_hub:\n",
    "    trainer.push_to_hub(dataset_name=dataset_id)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test and evaluate the aligned model\n",
    "\n",
    "After the training is done we want to evaluate and test our model. Similar to our SFT model, we will evaluate the model on [GSM8K](https://huggingface.co/datasets/openai/gsm8k) dataset to see if it improved performance. GSM8K (Grade School Math 8K) is a dataset of 8.5K high quality linguistically diverse grade school math word problems. The dataset was created to support the task of question answering on basic mathematical problems that require multi-step reasoning.\n",
    "\n",
    "Evaluating Generative AI models is not a trivial task since 1 input can have multiple correct outputs. If you want to learn more about evaluating generative models, check out:\n",
    "* [Evaluate LLMs and RAG a practical example using Langchain and Hugging Face](https://www.philschmid.de/evaluate-llm).\n",
    "* [Evaluate LLMs using Evaluation Harness and Hugging Face TGI/vLLM](https://www.philschmid.de/evaluate-llms-with-lm-eval-and-tgi-vllm)\n",
    "* [LLM Evaluation doesn't need to be complicated](https://www.philschmid.de/llm-evaluation)\n",
    "* [Evaluating Open LLMs with MixEval: The Closest Benchmark to LMSYS Chatbot Arena](https://www.philschmid.de/evaluate-llm-mixeval)\n",
    "\n",
    "We are going to use [Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) an open-source framework to evaluate language models on a wide range of tasks and benchmarks. The frameworks support evaluating models behind OpenAI compatible API endpoints, those can be locally or remotely. This super helpful as we can evaluate our model in the same environment we will use for production. \n",
    "\n",
    "\n",
    "We are going to use [Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference) for testing and deploying our model. TGI is a purpose-built solution for deploying and serving Large Language Models (LLMs). TGI enables high-performance text generation using Tensor Parallelism and continous batching. If you are or want to use vLLM you can check the Appendix on how to start the inference server.\n",
    "\n",
    "_Note: Make sure that you have enough GPU memory to run the container. Restart kernel to remove all allocated GPU memory from the notebook._ \n",
    "\n",
    "We will start the on 1 GPU detached. Meaning we can can continue to use the notebook while the container is running. If you have more GPUs you can change the `--gpus` and `--num-shard` flags to the number of GPUs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "num_gpus=1\n",
    "model_id=philschmid/dpo-llama-3-1-8b-math-ep3-merged # replace with your model id\n",
    "\n",
    "docker run --name tgi --gpus ${num_gpus} -d -ti -p 8080:80 --shm-size=2GB \\\n",
    "  -e HF_TOKEN=$(cat ~/.cache/huggingface/token) \\\n",
    "  ghcr.io/huggingface/text-generation-inference:3.0.1 \\\n",
    "  --model-id ${model_id} \\\n",
    "  --num-shard ${num_gpus}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our container will now start in the background and download the model from Hugging Face Hub. We can check the logs to see the progress with `docker logs -f tgi`.\n",
    "\n",
    "Once our container is running we can send requests using the `openai` or `huggingface_hub` sdk. Here we ll use the `openai` sdk to send a request to our inference server. If you don't have the `openai` sdk installed you can install it using `pip install openai`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# create client \n",
    "client = OpenAI(base_url=\"http://localhost:8080/v1\",api_key=\"-\")\n",
    "\n",
    "system_message = \"\"\"Solve the given high school math problem by providing a clear explanation of each step leading to the final solution.\n",
    "\n",
    "Provide a detailed breakdown of your calculations, beginning with an explanation of the problem and describing how you derive each formula, value, or conclusion. Use logical steps that build upon one another, to arrive at the final answer in a systematic manner.\n",
    "\n",
    "# Steps\n",
    "\n",
    "1. **Understand the Problem**: Restate the given math problem and clearly identify the main question and any important given values.\n",
    "2. **Set Up**: Identify the key formulas or concepts that could help solve the problem (e.g., algebraic manipulation, geometry formulas, trigonometric identities).\n",
    "3. **Solve Step-by-Step**: Iteratively progress through each step of the math problem, justifying why each consecutive operation brings you closer to the solution.\n",
    "4. **Double Check**: If applicable, double check the work for accuracy and sense, and mention potential alternative approaches if any.\n",
    "5. **Final Answer**: Provide the numerical or algebraic solution clearly, accompanied by appropriate units if relevant.\n",
    "\n",
    "# Notes\n",
    "\n",
    "- Always clearly define any variable or term used.\n",
    "- Wherever applicable, include unit conversions or context to explain why each formula or step has been chosen.\n",
    "- Assume the level of mathematics is suitable for high school, and avoid overly advanced math techniques unless they are common at that level.\n",
    "- Return the final in an extra line. Staring with \"The Answer is: [ANSWER]\"\n",
    "\n",
    "# Examples\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    # {\"role\": \"user\", \"content\": \"If you converted $140 to 158760 Korean Won, how much is $1 in Korean Won?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Q: Henry and 3 of his friends order 7 pizzas for lunch. Each pizza is cut into 8 slices. If Henry and his friends want to share the pizzas equally, how many slices can each of them have?\\nA:\"},\n",
    "    # {\"role\": \"user\", \"content\": \"The rectangular-shaped cell phone is 9 centimeters (cm) wide and 46 centimeters (cm) in circumference. Find the vertical length of the cell phone?\"},\n",
    "]\n",
    "expected_answer = \"14\"\n",
    "\n",
    "\n",
    "# Take a random sample from the dataset and remove the last message and send it to the model\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"philschmid/dpo-llama-3-1-8b-math-ep3-merged\",\n",
    "    messages=messages,\n",
    "    stream=False, # no streaming\n",
    "    max_tokens=1024,\n",
    "    temperature=1.0)\n",
    "response = response.choices[0].message.content\n",
    "\n",
    "# Print results\n",
    "print(f\"Query:\\n{messages[1]['content']}\")\n",
    "print(f\"Original Answer:\\n{expected_answer}\")\n",
    "print(f\"Generated Answer:\\n{response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome that looks great! Now we can evaluate our model with the [Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness).\n",
    "\n",
    "_Note: Make sure to change the model id to your fine-tuned model._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lm_eval --model local-chat-completions \\\n",
    "  --tasks gsm8k_cot \\\n",
    "  --model_args model=philschmid/dpo-llama-3-1-8b-math-ep3-merged,base_url=http://localhost:8080/v1/chat/completions,num_concurrent=8,max_retries=10,tokenized_requests=False,timeout=180,max_length=4096 \\\n",
    "  --apply_chat_template \\\n",
    "  --fewshot_as_multiturn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, 59% accuracy, thats a 5% improvement from our SFT model, using only ~2k preference pairs for 3 epochs. That shows that our script and config is working correctly. \n",
    "\n",
    "_Note: You might be able to achieve better results with more data, more epochs or tuning the hyperparameters (beta, learning rate, batch size, etc.). I ran some ablations on multi-gpu training and full training with DeepSpeed (see Appendix for full command) and the best results was 62% accuracy._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop tgi\n",
    "!docker rm tgi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "_Note: Make sure to install deepspeed and accelerate before running the commands. `pip install deepspeed==0.15.4`_\n",
    "\n",
    "\n",
    "## Distributed Training\n",
    "\n",
    "```bash\n",
    "ACCELERATE_LOG_LEVEL=info accelerate launch --num_processes 4 --config_file configs/accelerate_configs/deepspeed_zero3.yaml scripts/dpo/run_dpo.py --config receipes/dpo-llama-3-1-8b.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
